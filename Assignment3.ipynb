{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8168e533-2fa9-4ebb-8643-ca7d017370d6",
   "metadata": {},
   "source": [
    "## ASSIGNMENT-3\n",
    "\n",
    "TEAM-NAME:\n",
    "\n",
    "IDs: \n",
    "\n",
    "NAMEs: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc148a-92f2-4fda-9493-4a1b610dca95",
   "metadata": {},
   "source": [
    "### Instructions\n",
    " * Fill in the team name, id and names of all the teams members in the cell above.\n",
    " * Code must be written in Python in Jupyter Notebooks. We highly recommend using anaconda distribution or at the minimum, virtual environments for this assignment.\n",
    " * All the code and result files should be uploaded in the github classroom.\n",
    " * You can use the in-built methods and **unless explicitly mentioned**, don't need to code from scratch for this assignment. Make sure your code is modular.\n",
    " * All the representations are expected to be in a right-hand coordinate system. All the functions related to transformation matrices, quaternions, and 3D projection are expected to be coded by you.\n",
    " * You could split the Jupyter Notebook cells where TODO is written, but please try to avoid splitting/changing the structure of other cells.\n",
    " * All the visualization should be done inside the notebook unless specified otherwise.\n",
    " * Plagiarism will lead to heavy penalty.\n",
    " * Commit this notebook in the GitHub Classroom repo and any other results files under the result folder. \n",
    " * Commits past the deadline will not be considered.\n",
    " * Extensions will be granted using the extension policy only, so please go through the policy carefully and reach out to the TAs beforehand.\n",
    " * **Download the data for section 2 and 3 from this [link](https://iiitaphyd-my.sharepoint.com/:f:/g/personal/pranjali_pathre_research_iiit_ac_in/Elm4OCxD4VhBh6f9x2ufKsgBnWwkmWc-nXPjnS5jXWRTww?e=IzOlbb)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c2a740-8793-4982-b720-55bde0cce863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee2ec4-3d41-4d18-836f-b573c0775f27",
   "metadata": {},
   "source": [
    "### SECTION 1: Epipolar lines and Epipoles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d41dd-ecb7-4af7-93d8-6e4f1e325577",
   "metadata": {},
   "source": [
    "For this task, you have been given two images of the same scene taken from different view-points. You should first estimate the fundamental matrix that encodes their relative geometry from these two images.\n",
    "\n",
    "<img src=\"data/img1.jpg\" alt=\"image 1\" width=\"400\"/>\n",
    "<img src=\"data/img2.jpg\" alt=\"image 2\" width=\"400\"/>\n",
    "\n",
    "Recall that given a point in one image, it's corresponding location in the other image can be found to along a line viz. the epipolar line. The task is to draw the epipolar lines in the second image for each given point in the first image. You have to repeat this for the other image as well. Draw epipolar lines on the first image for the corresponding points in the second image.\n",
    "\n",
    "The convention used for F is $x'^{T}Fx$ where $x'$ is the location of the point in the second image. For this question you will need to compute the F matrix on your own without using inbuilt functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9128d454-4358-44ba-b715-965a3ab63ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 1\n",
    "##############################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "##############################################################################\n",
    "# END OF YOUR CODE\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab4b45-6429-4188-a5c0-cc1816ac6622",
   "metadata": {},
   "source": [
    "### SECTION 2: Visual Odometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73085ce-9943-4a3f-a0b7-4b2c6692d59d",
   "metadata": {},
   "source": [
    "Visual odometry is the process of recovering the egomotion (the trajectory) of an agent using only the input from the camera or a system of cameras attached to the agent. This is a well-\n",
    "studied problem in robotic vision and is a critical part of many applications such as mars rovers, and self-driving cars for localization. You will be implementing a basic monocular visual odometry algorithm in this part of the assignment.\n",
    "\n",
    "To begin with, download all the required data from `data/2`. It contains a sequence of images from the KITTI dataset. The ground truth pose of each frame (in row-major order) and the camera parameters are provided as well.\n",
    "\n",
    "#### Procedure\n",
    "The following is an overview of the entire algorithm. \n",
    "1. Find the corresponding features between frames $I_{k}$ and $I_{k-1}$.\n",
    "2. Using these features correspondances, estimate the essential matrix between the two images within a RANSAC sceme.\n",
    "3. Decompose the essential matrix to obtain the relative rotation $R_k$ and transition $t_k$, and form the transformation $T_k$.\n",
    "4. Scale the translation $t_k$ with the absolute or the relative scale. \n",
    "5. Concatenate the relative transformation by computing $C_k$ = $C_{k-1}$ $T_k$, where $C_{k-1}$ is the previous pose of the camera in the world frame. \n",
    "6. Repeat the above steps for the remaining pairs of frames.\n",
    "\n",
    "The main task in computing visual odometry is to compute the relative transformation $T_k$ from each pair of images $I_k$ and $I_{k-1}$ and then to concatenate these transformations to recover the full trajectory of the camera. There are two broad approaches to compute the relative motion $T_k$: Appearance-based (or direct) method, which uses the intensity information of all the pixels in the input images and feature-based method which only uses salient and repeatable features extracted and tracked across the images. You will be implementing a feature-based method. \n",
    "\n",
    "For every new image $I_k$, the first step consists of detecting and matching 2D features with those from the previous frame. These 2D features are the locations in the image which we can reliably find in multiple images and possibly match them. To find these features use the inbuilt SIFT OpenCV implementation. \n",
    "\n",
    "Refer to [OpenCV documentation](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html) for the impementation details. \n",
    "\n",
    "As mentioned earlier, the main task is motion computation. Using these feature correspondences, implement the 8-point algorithm for fundamental matrix estimation. Implement it inside a RANSAC scheme to get rid of any outliers, as explained in class. Then, compute the essential matrix, and decompose it to the relative R and t using ```cv2.recoverPose```. Note that the function returns the R and t of the first camera with respect to the second, and not the other way around.\n",
    "\n",
    "Now, you might recall that the absolute scale of the translation cannot be computed from just two images. The above function only returns the direction of t, as a unit vector. Use the ground truth translation to get the absolute scale, and multiply your unit translation with this scale. Then concatenate your transformations, and repeat for the next pair of frames to recover the full absolute trajector\n",
    "\n",
    "Deliverables:\n",
    "* A .txt file containing the estimated poses, provided in the same format as the ground truth.\n",
    "* A plot of the estimated trajectory along with the ground truth trajectory. Also report the obtained trajectory error. Use [EVO](https://github.com/MichaelGrupp/evo) for this.\n",
    "```\n",
    "pip install evo --upgrade --no-binary evo\n",
    "evo_traj kitti ground-truth.txt your-result.txt -va --plot --plot_mode xz\n",
    "```\n",
    "* Comment on the performance of your algorithm. When does it work well or it fails and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4009a13a-018c-4d8b-9464-dea7fd1e61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in section 2\n",
    "##############################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "##############################################################################\n",
    "# END OF YOUR CODE\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16719ee-18f9-46d8-b4b7-8dbf8323c47f",
   "metadata": {},
   "source": [
    "### SECTION 3: Stereo Dense Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ac1cc-58d5-48a5-b7ed-6ab90ac1482e",
   "metadata": {},
   "source": [
    "3-D point clouds are very useful in robotics for several tasks such as object detection, motion estimation (3D-3D matching or 3D-2D matching), SLAM, and other forms of scene understanding.  Stereo cameras provide  us  with  a  convenient  way  to  generate  dense  point  clouds. Dense here,  in  contrast  to sparse, means all the image points are used for the reconstruction.  In this part of the assignment you will be generating a dense 3D point cloud reconstruction of a scene from stereo images. Refer `data/3` folder. It contains `img2` and `img3` folder for left and right stereo images respectively. `poses.txt` contains flatten 12 values of transformation matrix for each pose and `calib.txt` contains `K & Baseline`.\n",
    "\n",
    "#### Procedure\n",
    "\n",
    "1. Generate a disparity map for all given stereo pair.  Use OpenCV (e.g.  StereoSGBM) for this.  Note that the images provided are already rectified and undistorted.\n",
    "2. Then, using the camera parameters and baseline information generate colored point clouds from each disparity map.  Some points will have invalid disparity values, so ignore them. Use `Open3D` for storing your point clouds.\n",
    "3. Register (or transform) all the generated point clouds into your world frame by using the provided ground truth poses.\n",
    "4. Visualize the registered point cloud data, in color.  Use Open3D for this.\n",
    "\n",
    "    \n",
    "    \n",
    "Write briefly about how the disparity map is generated (if you used SGBM, write about SGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e58c28d-0dd1-4db8-9c90-7c8d7a61d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in section 3\n",
    "##############################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "##############################################################################\n",
    "# END OF YOUR CODE\n",
    "##############################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
